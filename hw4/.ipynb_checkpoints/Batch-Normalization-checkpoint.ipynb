{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "\n",
    "In this notebook, you will implement the batch normalization layers of a neural network to increase its performance.  If you have any confusion, please review the details of batch normalization from the lecture notes.\n",
    "\n",
    "CS231n has built a solid API for building these modular frameworks and training them, and we will use their very well implemented framework as opposed to \"reinventing the wheel.\"  This includes using their Solver, various utility functions, and their layer structure.  This also includes nndl.fc_net, nndl.layers, and nndl.layer_utils.  As in prior assignments, we thank Serena Yeung & Justin Johnson for permission to use code written for the CS 231n class (cs231n.stanford.edu).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import and setups\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nndl.fc_net import *\n",
    "from nndl.layers import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (49000, 3, 32, 32) \n",
      "y_train: (49000,) \n",
      "X_val: (1000, 3, 32, 32) \n",
      "y_val: (1000,) \n",
      "X_test: (1000, 3, 32, 32) \n",
      "y_test: (1000,) \n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k in data.keys():\n",
    "  print('{}: {} '.format(k, data[k].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batchnorm forward pass\n",
    "\n",
    "Implement the training time batchnorm forward pass, `batchnorm_forward`, in `nndl/layers.py`. After that, test your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before batch normalization:\n",
      "  means:  [ -5.15168248   1.65041669 -21.95305926]\n",
      "  stds:  [ 31.02626866  38.61063106  34.5456651 ]\n",
      "After batch normalization (gamma=1, beta=0)\n",
      "  mean:  [  5.10702591e-17   1.89431804e-17   2.02615702e-16]\n",
      "  std:  [ 0.99999999  1.          1.        ]\n",
      "After batch normalization (nontrivial gamma, beta)\n",
      "  means:  [ 11.  12.  13.]\n",
      "  stds:  [ 0.99999999  1.99999999  2.99999999]\n"
     ]
    }
   ],
   "source": [
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after batch normalization\n",
    "\n",
    "# Simulate the forward pass for a two-layer network\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "print('Before batch normalization:')\n",
    "print('  means: ', a.mean(axis=0))\n",
    "print('  stds: ', a.std(axis=0))\n",
    "\n",
    "# Means should be close to zero and stds close to one\n",
    "print('After batch normalization (gamma=1, beta=0)')\n",
    "a_norm, _ = batchnorm_forward(a, np.ones(D3), np.zeros(D3), {'mode': 'train'})\n",
    "print('  mean: ', a_norm.mean(axis=0))\n",
    "print('  std: ', a_norm.std(axis=0))\n",
    "\n",
    "# Now means should be close to beta and stds close to gamma\n",
    "gamma = np.asarray([1.0, 2.0, 3.0])\n",
    "beta = np.asarray([11.0, 12.0, 13.0])\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print('After batch normalization (nontrivial gamma, beta)')\n",
    "print('  means: ', a_norm.mean(axis=0))\n",
    "print('  stds: ', a_norm.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the testing time batchnorm forward pass, `batchnorm_forward`, in `nndl/layers.py`. After that, test your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch normalization (test-time):\n",
      "  means:  [-0.09680516 -0.02744277 -0.04458911]\n",
      "  stds:  [ 0.98422417  1.03874046  0.99286409]\n"
     ]
    }
   ],
   "source": [
    "# Check the test-time forward pass by running the training-time\n",
    "# forward pass many times to warm up the running averages, and then\n",
    "# checking the means and variances of activations after a test-time\n",
    "# forward pass.\n",
    "\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "gamma = np.ones(D3)\n",
    "beta = np.zeros(D3)\n",
    "for t in np.arange(50):\n",
    "  X = np.random.randn(N, D1)\n",
    "  a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "  batchnorm_forward(a, gamma, beta, bn_param)\n",
    "bn_param['mode'] = 'test'\n",
    "X = np.random.randn(N, D1)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, bn_param)\n",
    "\n",
    "# Means should be close to zero and stds close to one, but will be\n",
    "# noisier than training-time forward passes.\n",
    "print('After batch normalization (test-time):')\n",
    "print('  means: ', a_norm.mean(axis=0))\n",
    "print('  stds: ', a_norm.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batchnorm backward pass\n",
    "\n",
    "Implement the backward pass for the batchnorm layer, `batchnorm_backward` in `nndl/layers.py`.  Check your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  8.81946538941e-09\n",
      "dgamma error:  1.47787175916e-11\n",
      "dbeta error:  5.6041780225e-12\n"
     ]
    }
   ],
   "source": [
    "# Gradient check batchnorm backward pass\n",
    "\n",
    "N, D = 4, 5\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "fx = lambda x: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fg = lambda a: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fb = lambda b: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta, dout)\n",
    "\n",
    "_, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "dx, dgamma, dbeta = batchnorm_backward(dout, cache)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dgamma error: ', rel_error(da_num, dgamma))\n",
    "print('dbeta error: ', rel_error(db_num, dbeta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a fully connected neural network with batchnorm layers\n",
    "\n",
    "Modify the `FullyConnectedNet()` class in `nndl/fc_net.py` to incorporate batchnorm layers.  You will need to modify the class in the following areas:\n",
    "\n",
    "(1) The gammas and betas need to be initialized to 1's and 0's respectively in `__init__`.\n",
    "\n",
    "(2) The `batchnorm_forward` layer needs to be inserted between each affine and relu layer (except in the output layer) in a forward pass computation in `loss`.  You may find it helpful to write an `affine_batchnorm_relu()` layer in `nndl/layer_utils.py` although this is not necessary.\n",
    "\n",
    "(3) The `batchnorm_backward` layer has to be appropriately inserted when calculating gradients.\n",
    "\n",
    "After you have done the appropriate modifications, check your implementation by running the following cell.\n",
    "\n",
    "Note, while the relative error for W3 should be small, as we backprop gradients more, you may find the relative error increases.  Our relative error for W1 is on the order of 1e-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running check with reg =  0\n",
      "Initial loss:  2.40282299678\n",
      "W1 relative error: 9.166347855591001e-05\n",
      "W2 relative error: 1.9031890428675527e-05\n",
      "W3 relative error: 3.9489331170859923e-10\n",
      "b1 relative error: 1.1102230246251565e-08\n",
      "b2 relative error: 4.440892098500626e-08\n",
      "b3 relative error: 1.3326580199118108e-10\n",
      "\n",
      "\n",
      "Running check with reg =  3.14\n",
      "Initial loss:  6.90417637112\n",
      "W1 relative error: 1.7258190517681128e-06\n",
      "W2 relative error: 7.625398036821881e-07\n",
      "W3 relative error: 3.1971150710672253e-08\n",
      "b1 relative error: 0.00444084768957964\n",
      "b2 relative error: 2.220446049250313e-08\n",
      "b3 relative error: 2.743488798257346e-10\n"
     ]
    }
   ],
   "source": [
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "  print('Running check with reg = ', reg)\n",
    "  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float64,\n",
    "                            use_batchnorm=True)\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print('Initial loss: ', loss)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print('{} relative error: {}'.format(name, rel_error(grad_num, grads[name])))\n",
    "  if reg == 0: print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a deep fully connected network with batch normalization.\n",
    "\n",
    "To see if batchnorm helps, let's train a deep neural network with and without batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-58-c3251ec63668>, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-58-c3251ec63668>\"\u001b[0;36m, line \u001b[0;32m29\u001b[0m\n\u001b[0;31m    tim_config={\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Try training a very deep net with batchnorm\n",
    "hidden_dims = [100, 100, 100, 100, 100]\n",
    "\n",
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "weight_scale = 2e-2\n",
    "bn_model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=True)\n",
    "model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=False)\n",
    "\n",
    "bn_solver = Solver(bn_model, small_data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=200)\n",
    "bn_solver.train()\n",
    "\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=200)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:106: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'solver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-8e49aa315b6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'baseline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbn_solver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'batchnorm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'solver' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1132f7390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(solver.loss_history, 'o', label='baseline')\n",
    "plt.plot(bn_solver.loss_history, 'o', label='batchnorm')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(solver.train_acc_history, '-o', label='baseline')\n",
    "plt.plot(bn_solver.train_acc_history, '-o', label='batchnorm')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(solver.val_acc_history, '-o', label='baseline')\n",
    "plt.plot(bn_solver.val_acc_history, '-o', label='batchnorm')\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batchnorm and initialization\n",
    "\n",
    "The following cells run an experiment where for a deep network, the initialization is varied.  We do training for when batchnorm layers are and are not included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running weight scale 1 / 20\n",
      "W1\n",
      "[[  1.16930329e-04   1.70088560e-06   2.73649857e-05 ...,   1.88885868e-04\n",
      "    9.70826295e-05   6.99560624e-05]\n",
      " [  5.64389552e-07   1.23933423e-04   5.69131262e-05 ...,  -1.08884626e-04\n",
      "    1.47632280e-04  -2.06986315e-05]\n",
      " [  1.52994224e-04   6.23853775e-05  -3.78017103e-05 ...,  -5.65675546e-05\n",
      "    4.32024826e-05   1.42925797e-04]\n",
      " ..., \n",
      " [  6.73565319e-06   2.56822503e-04   1.89000581e-04 ...,  -8.28263364e-05\n",
      "    9.75325092e-05   2.18845642e-04]\n",
      " [ -8.70995282e-05   4.88762407e-06  -1.74858505e-04 ...,   1.26322921e-04\n",
      "   -8.02211332e-07  -1.05936837e-04]\n",
      " [ -2.05477714e-04   6.07012153e-05   1.02671365e-04 ...,   1.11328682e-05\n",
      "   -6.21028012e-05   1.73956723e-05]]\n",
      "b1\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "gamma1\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'gamma1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-6ed590e5c24d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m                   },\n\u001b[1;32m     26\u001b[0m                   verbose=False, print_every=200)\n\u001b[0;32m---> 27\u001b[0;31m   \u001b[0mbn_solver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m   \u001b[0mbn_solvers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweight_scale\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbn_solver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/2018 Winter/ECE 239AS/hw4 code/cs231n/solver.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;31m# Maybe print training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/2018 Winter/ECE 239AS/hw4 code/cs231n/solver.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim_configs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mnext_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gamma1'"
     ]
    }
   ],
   "source": [
    "# Try training a very deep net with batchnorm\n",
    "hidden_dims = [50, 50, 50, 50, 50, 50, 50]\n",
    "\n",
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "bn_solvers = {}\n",
    "solvers = {}\n",
    "weight_scales = np.logspace(-4, 0, num=20)\n",
    "for i, weight_scale in enumerate(weight_scales):\n",
    "  print('Running weight scale {} / {}'.format(i + 1, len(weight_scales)))\n",
    "  bn_model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=True)\n",
    "  model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=False)\n",
    "\n",
    "  bn_solver = Solver(bn_model, small_data,\n",
    "                  num_epochs=10, batch_size=50,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-3,\n",
    "                  },\n",
    "                  verbose=False, print_every=200)\n",
    "  bn_solver.train()\n",
    "  bn_solvers[weight_scale] = bn_solver\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=10, batch_size=50,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-3,\n",
    "                  },\n",
    "                  verbose=False, print_every=200)\n",
    "  solver.train()\n",
    "  solvers[weight_scale] = solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results of weight scale experiment\n",
    "best_train_accs, bn_best_train_accs = [], []\n",
    "best_val_accs, bn_best_val_accs = [], []\n",
    "final_train_loss, bn_final_train_loss = [], []\n",
    "\n",
    "for ws in weight_scales:\n",
    "  best_train_accs.append(max(solvers[ws].train_acc_history))\n",
    "  bn_best_train_accs.append(max(bn_solvers[ws].train_acc_history))\n",
    "  \n",
    "  best_val_accs.append(max(solvers[ws].val_acc_history))\n",
    "  bn_best_val_accs.append(max(bn_solvers[ws].val_acc_history))\n",
    "  \n",
    "  final_train_loss.append(np.mean(solvers[ws].loss_history[-100:]))\n",
    "  bn_final_train_loss.append(np.mean(bn_solvers[ws].loss_history[-100:]))\n",
    "  \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Best val accuracy vs weight initialization scale')\n",
    "plt.xlabel('Weight initialization scale')\n",
    "plt.ylabel('Best val accuracy')\n",
    "plt.semilogx(weight_scales, best_val_accs, '-o', label='baseline')\n",
    "plt.semilogx(weight_scales, bn_best_val_accs, '-o', label='batchnorm')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Best train accuracy vs weight initialization scale')\n",
    "plt.xlabel('Weight initialization scale')\n",
    "plt.ylabel('Best training accuracy')\n",
    "plt.semilogx(weight_scales, best_train_accs, '-o', label='baseline')\n",
    "plt.semilogx(weight_scales, bn_best_train_accs, '-o', label='batchnorm')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Final training loss vs weight initialization scale')\n",
    "plt.xlabel('Weight initialization scale')\n",
    "plt.ylabel('Final training loss')\n",
    "plt.semilogx(weight_scales, final_train_loss, '-o', label='baseline')\n",
    "plt.semilogx(weight_scales, bn_final_train_loss, '-o', label='batchnorm')\n",
    "plt.legend()\n",
    "\n",
    "plt.gcf().set_size_inches(10, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "In the cell below, summarize the findings of this experiment, and WHY these results make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
